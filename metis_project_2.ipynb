{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model of Housing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to relate housing prices to the natural amenities found in a given county. I will be using a dataset called the Natural Amenities Scale, which can be found [here](https://www.ers.usda.gov/data-products/natural-amenities-scale.aspx)\n",
    "\n",
    "I will also be using a dataset of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing natural amenity csv\n",
    "nat = pd.read_csv('/Users/patricknorman/Documents/GitHub/metis-project-2/data/nat_amen_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>COMBINED_COUNTY_INDEX</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CENSUS_DIV</th>\n",
       "      <th>RUC</th>\n",
       "      <th>UIC</th>\n",
       "      <th>JAN_TEMP</th>\n",
       "      <th>JAN_SUN</th>\n",
       "      <th>JULY_TEMP</th>\n",
       "      <th>...</th>\n",
       "      <th>WATER</th>\n",
       "      <th>LN_WATER</th>\n",
       "      <th>JAN TEMP - Z</th>\n",
       "      <th>JAN SUN - Z</th>\n",
       "      <th>JUL TEMP - Z</th>\n",
       "      <th>JUL HUM - Z</th>\n",
       "      <th>TOPOG - Z</th>\n",
       "      <th>LN WATER  AREA - Z</th>\n",
       "      <th>NAT_AMENITY</th>\n",
       "      <th>RANK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>AL</td>\n",
       "      <td>AUTAUGA</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>130</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>4.944</td>\n",
       "      <td>1.20277</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.13206</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>0.26585</td>\n",
       "      <td>0.78</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>51.9</td>\n",
       "      <td>152</td>\n",
       "      <td>80.6</td>\n",
       "      <td>...</td>\n",
       "      <td>21.24</td>\n",
       "      <td>7.661</td>\n",
       "      <td>1.57504</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.36308</td>\n",
       "      <td>-1.09576</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.70428</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>AL</td>\n",
       "      <td>BARBOUR</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>48.8</td>\n",
       "      <td>152</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.378</td>\n",
       "      <td>1.31859</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>-0.01148</td>\n",
       "      <td>-0.89039</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.49570</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>1007</td>\n",
       "      <td>AL</td>\n",
       "      <td>BIBB</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>44.8</td>\n",
       "      <td>130</td>\n",
       "      <td>80.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.916</td>\n",
       "      <td>0.98768</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.30223</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>-0.27812</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>1009</td>\n",
       "      <td>AL</td>\n",
       "      <td>BLOUNT</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41.9</td>\n",
       "      <td>130</td>\n",
       "      <td>78.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4.336</td>\n",
       "      <td>0.74778</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>0.09330</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>-0.05592</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS  COMBINED_COUNTY_INDEX STATE   COUNTY  CENSUS_DIV  RUC  UIC  JAN_TEMP  \\\n",
       "0  1001                   1001    AL  AUTAUGA           6    2    2      47.4   \n",
       "1  1003                   1003    AL  BALDWIN           6    2    2      51.9   \n",
       "2  1005                   1005    AL  BARBOUR           6    6    5      48.8   \n",
       "3  1007                   1007    AL     BIBB           6    6    6      44.8   \n",
       "4  1009                   1009    AL   BLOUNT           6    2    2      41.9   \n",
       "\n",
       "   JAN_SUN  JULY_TEMP  ...  WATER  LN_WATER  JAN TEMP - Z  JAN SUN - Z  \\\n",
       "0      130       81.0  ...   1.40     4.944       1.20277     -0.64758   \n",
       "1      152       80.6  ...  21.24     7.661       1.57504      0.01482   \n",
       "2      152       81.0  ...   2.17     5.378       1.31859      0.01482   \n",
       "3      130       80.8  ...   0.50     3.916       0.98768     -0.64758   \n",
       "4      130       78.4  ...   0.76     4.336       0.74778     -0.64758   \n",
       "\n",
       "   JUL TEMP - Z  JUL HUM - Z  TOPOG - Z  LN WATER  AREA - Z  NAT_AMENITY  RANK  \n",
       "0      -0.13206     -0.68502    0.77701             0.26585         0.78     4  \n",
       "1       0.36308     -1.09576   -0.73966             1.70428         1.82     4  \n",
       "2      -0.01148     -0.89039   -0.73966             0.49570         0.19     4  \n",
       "3      -0.30223     -0.68502    0.77701            -0.27812        -0.15     3  \n",
       "4       0.09330     -0.68502    0.77701            -0.05592         0.23     4  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying sucessful import\n",
    "nat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clarifying some column names\n",
    "nat.rename(columns={'RUC':'RUC_93','UIC':'UIC_93'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3111, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining shape\n",
    "nat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data\n",
    "I've found a good housing cost dataset for 2013 [here](https://data.census.gov/cedsci/table?q=home%20values%202013%20county&t=Housing&g=0100000US.050000&tid=ACSST1Y2013.S2506&hidePreview=true). I just have to import it and wrangle with the insane format.\n",
    "\n",
    "Note: Don't be alarmed, this is just a list of 495 counties. It's supposedly a representative sample according to the Census..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing housing df\n",
    "housing_raw = pd.read_csv('/Users/patricknorman/Documents/GitHub/metis-project-2/data/2013_housing.csv')\n",
    "#housing_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting housing df\n",
    "housing = housing_raw.transpose()\n",
    "housing.columns = housing.iloc[0]\n",
    "housing.drop(labels=['Label'],inplace=True)\n",
    "housing.reset_index(inplace=True)\n",
    "housing.rename(columns={\n",
    "    'index':'COUNTY',\n",
    "    'Owner-occupied housing units with a mortgage':'N'},\n",
    "               inplace=True)\n",
    "\n",
    "housing.COUNTY = housing.COUNTY.replace({'!!Owner-occupied housing units with a mortgage!!':' ',\n",
    "                                        'Estimate':'EST','Margin of Error':'MAR',\n",
    "                                        'County,':'',\n",
    "                                        ',':''}, regex=True,)\n",
    "\n",
    "state_abb = {'Alabama':'AL','Alaska': 'AK','Arizona':'AZ','Arkansas':'AR',\n",
    "             'California':'CA','Colorado':'CO','Conneticut':'CT','Delaware':'DE',\n",
    "             'Florida':'FL','Georgia':'GA','Hawaii':'HI','Idaho':'ID','Illinois':'IL',\n",
    "             'Indiana':'IN','Iowa':'IA','Kansas':'KS','Kentucky':'KY','Louisiana':'LA',\n",
    "             'Maine':'ME','Maryland':'MD','Massachusetts':'MA','Michigan':'MI','Minnesota':'MN',\n",
    "             'Mississippi':'MS','Missouri':'MO','Montana':'MT','Nebraska':'NE',\n",
    "             'Nevada':'NV','New Hampshire':'NH','New Jersey':'NJ','New Mexico':'NM',\n",
    "             'New York':'NY','North Carolina':'NC','North Dakota':'ND','Ohio':'OH',\n",
    "             'Oklahoma':'OK','Oregon':'OR','Pennsylvania':'PA','Rhode Island':'RI',\n",
    "             'South Carolina':'SC','South Dakota':'SD','Tennessee':'TN','Texas':'TX',\n",
    "             'Utah':'UT','Vermont':'VT','West Virginia':'WV','Virginia':'VA','Washington':'WA',\n",
    "             'Wisconsin':'WI','Wyoming':'WY'}\n",
    "\n",
    "housing.COUNTY = housing.COUNTY.replace(state_abb,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column for STATE\n",
    "housing['STATE'] = housing.COUNTY.apply(lambda x: str(x)[-6:-3])\n",
    "\n",
    "housing['STATE'] = housing['STATE'].str.upper()\n",
    "#housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COUNTY', 'N', 'VALUE', 'Less than $50,000', '$50,000 to $99,999',\n",
       "       '$100,000 to $149,999', '$150,000 to $199,999', '$200,000 to $299,999',\n",
       "       '$300,000 to $499,999', '$500,000 or more', 'MEDIAN_VAL',\n",
       "       'MORTGAGE STATUS',\n",
       "       '    With either a second mortgage, or home equity loan, but not both',\n",
       "       '        Second mortgage only', '        Home equity loan only',\n",
       "       '    Both second mortgage and home equity loan',\n",
       "       '    No second mortgage and no home equity loan',\n",
       "       'HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2013 INFLATION-ADJUSTED DOLLARS)',\n",
       "       'Less than $10,000', '$10,000 to $24,999', '$25,000 to $34,999',\n",
       "       '$35,000 to $49,999', '$50,000 to $74,999', '$75,000 to $99,999',\n",
       "       '$100,000 to $149,999', '$150,000 or more',\n",
       "       '    Median household income (dollars)',\n",
       "       'RATIO OF VALUE TO HOUSEHOLD INCOME IN THE PAST 12 MONTHS',\n",
       "       'Less than 2.0', '2.0 to 2.9', '3.0 to 3.9', '4.0 or more',\n",
       "       '    Not computed', 'MONTHLY HOUSING COSTS', 'Less than $200',\n",
       "       '$200 to $299', '$300 to $399', '$400 to $499', '$500 to $599',\n",
       "       '$600 to $699', '$700 to $799', '$800 to $899', '$900 to $999',\n",
       "       '$1,000 to $1,249', '$1,250 to $1,499', '$1,500 to $1,999',\n",
       "       '$2,000 or more', '    Median (dollars)',\n",
       "       'MONTHLY HOUSING COSTS AS A PERCENTAGE OF HOUSEHOLD INCOME IN THE PAST 12 MONTHS',\n",
       "       'Less than $20,000', 'Less than 20 percent', '20 to 29 percent',\n",
       "       '30 percent or more', '$20,000 to $34,999', 'Less than 20 percent',\n",
       "       '20 to 29 percent', '30 percent or more', '$35,000 to $49,999',\n",
       "       'Less than 20 percent', '20 to 29 percent', '30 percent or more',\n",
       "       '$50,000 to $74,999', 'Less than 20 percent', '20 to 29 percent',\n",
       "       '30 percent or more', '$75,000 or more', 'Less than 20 percent',\n",
       "       '20 to 29 percent', '30 percent or more', '    Zero or negative income',\n",
       "       'REAL ESTATE TAXES', 'Less than $800', '$800 to $1,499',\n",
       "       '$1,500 or more', '    No real estate taxes paid',\n",
       "       '    Median (dollars)', 'STATE'],\n",
       "      dtype='object', name='Label')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Label</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>N</th>\n",
       "      <th>STATE</th>\n",
       "      <th>MEDIAN_VAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Rutherford  NC EST</td>\n",
       "      <td>8,552</td>\n",
       "      <td>NC</td>\n",
       "      <td>115,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Rutherford  NC MAR</td>\n",
       "      <td>±1,169</td>\n",
       "      <td>NC</td>\n",
       "      <td>±19,775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Pasco  FL EST</td>\n",
       "      <td>72,592</td>\n",
       "      <td>FL</td>\n",
       "      <td>143,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Pasco  FL MAR</td>\n",
       "      <td>±2,756</td>\n",
       "      <td>FL</td>\n",
       "      <td>±7,379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Jasper  MO EST</td>\n",
       "      <td>17,315</td>\n",
       "      <td>MO</td>\n",
       "      <td>118,300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Label              COUNTY       N STATE MEDIAN_VAL\n",
       "994    Rutherford  NC EST   8,552   NC     115,200\n",
       "995    Rutherford  NC MAR  ±1,169   NC     ±19,775\n",
       "996         Pasco  FL EST  72,592   FL     143,600\n",
       "997         Pasco  FL MAR  ±2,756   FL      ±7,379\n",
       "998        Jasper  MO EST  17,315   MO     118,300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before I continue, let's get rid of some of the columns that i dont need\n",
    "house = housing[['COUNTY','N','STATE','MEDIAN_VAL']] \n",
    "house.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-07206fe23c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhouse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MEDIAN_VAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.savefig('housing_hist.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(house['MEDIAN_VAL'])\n",
    "\n",
    "#plt.savefig('housing_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['224,000', '±20,267', '145,900', '±42,512', '173,600', '±14,461',\n",
       "       '132,900', '±16,065', '144,900', '±10,321', '187,400', '±12,999',\n",
       "       '158,100', '±9,955', '176,000', '±21,072', '339,000', '±22,432',\n",
       "       '261,400', '±11,629', '120,200', '±13,445', '223,300', '±13,861',\n",
       "       '141,100', '±25,252', '105,300', '±41,798', '131,800', '±26,407',\n",
       "       '441,000', '±5,160', '363,800', '±14,380', '416,200', '±11,778',\n",
       "       '508,900', '±17,324', '164,700', '±6,646', '226,500', '±22,861',\n",
       "       '348,900', '±37,854', '129,700', '±14,525', '102,700', '±15,010',\n",
       "       '99,100', '±12,071', '103,600', '±33,704', '153,900', '±21,977',\n",
       "       '172,600', '±9,543', '100,000', '±12,514', '119,400', '±8,725',\n",
       "       '133,200', '±20,666', '598,500', '±25,988', '354,300', '±21,900',\n",
       "       '297,600', '±23,887', '213,900', '±26,052', '304,800', '±20,805',\n",
       "       '463,800', '±32,176', '168,000', '±6,668', '130,200', '±19,777',\n",
       "       '141,800', '±16,153', '117,000', '±6,518', '146,000', '±7,459',\n",
       "       '244,600', '±13,362', '154,700', '±7,503', '97,900', '±12,162',\n",
       "       '111,800', '±13,004', '168,200', '±12,644', '146,700', '±6,525',\n",
       "       '122,100', '±7,613', '208,600', '±4,870', '192,000', '±8,761',\n",
       "       '130,700', '±6,261', '145,000', '±12,197', '112,100', '±14,180',\n",
       "       '146,100', '±12,451', '209,600', '±12,786', '89,800', '±3,803',\n",
       "       '196,800', '±14,190', '244,200', '±5,491', '111,400', '±14,646',\n",
       "       '143,300', '±17,219', '153,300', '±8,951', '162,300', '±6,947',\n",
       "       '140,500', '±8,494', '131,900', '±21,984', '156,100', '±10,390',\n",
       "       '158,100', '±16,627', '192,200', '±11,460', '158,500', '±6,085',\n",
       "       '223,800', '±4,768', '165,100', '±4,116', '245,400', '±13,720',\n",
       "       '145,700', '±8,791', '265,400', '±7,512', '121,200', '±3,166',\n",
       "       '124,800', '±10,701', '104,000', '±4,804', '208,000', '±9,946',\n",
       "       '247,600', '±13,390', '124,200', '±13,459', '130,900', '±13,645',\n",
       "       '167,600', '±7,260', '149,100', '±12,294', '156,200', '±16,287',\n",
       "       '429,100', '±13,385', '139,300', '±10,856', '113,200', '±8,528',\n",
       "       '205,900', '±17,773', '196,400', '±2,700', '221,200', '±5,159',\n",
       "       '422,600', '±3,914', '176,300', '±8,172', '165,600', '±7,517',\n",
       "       '182,500', '±10,460', '449,200', '±6,980', '181,000', '±12,990',\n",
       "       '154,200', '±5,055', '149,500', '±9,154', '144,300', '±11,436',\n",
       "       '217,700', '±7,647', '158,700', '±17,491', '139,200', '±6,822',\n",
       "       '491,400', '±6,790', '148,100', '±13,435', '152,600', '±6,760',\n",
       "       '112,300', '±10,450', '236,700', '±10,281', '188,100', '±8,898',\n",
       "       '801,500', '±21,162', '151,900', '±7,560', '167,100', '±10,844',\n",
       "       '299,800', '±28,671', '155,700', '±4,551', '459,800', '±9,034',\n",
       "       '214,000', '±7,069', '108,300', '±24,486', '208,600', '±13,797',\n",
       "       '88,500', '±3,201', '185,400', '±10,411', '248,500', '±5,746',\n",
       "       '241,600', '±7,466', '152,000', '±12,088', '194,700', '±7,376',\n",
       "       '152,100', '±6,622', '156,400', '±8,901', '221,600', '±7,492',\n",
       "       '132,400', '±5,245', '161,800', '±10,203', '201,300', '±12,257',\n",
       "       '159,200', '±7,588', '118,300', '±16,749', '161,900', '±6,594',\n",
       "       '267,300', '±13,071', '176,300', '±4,631', '139,900', '±4,738',\n",
       "       '294,300', '±8,440', '208,900', '±6,864', '118,700', '±3,715',\n",
       "       '190,100', '±15,223', '141,200', '±6,316', '259,700', '±11,735',\n",
       "       '257,600', '±7,487', '229,700', '±13,556', '145,800', '±6,312',\n",
       "       '146,500', '±5,384', '252,600', '±11,541', '277,900', '±8,700',\n",
       "       '141,700', '±10,222', '170,100', '±5,150', '191,900', '±8,611',\n",
       "       '124,400', '±12,058', '99,500', '±7,887', '108,700', '±8,073',\n",
       "       '150,900', '±9,575', '311,000', '±19,520', '162,900', '±7,946',\n",
       "       '135,000', '±8,176', '153,400', '±4,051', '116,100', '±11,963',\n",
       "       '115,300', '±9,010', '107,600', '±7,359', '112,800', '±8,237',\n",
       "       '142,100', '±6,095', '128,600', '±7,781', '149,200', '±11,397',\n",
       "       '250,900', '±14,300', '168,000', '±7,092', '171,200', '±5,667',\n",
       "       '196,700', '±6,001', '130,600', '±7,201', '161,200', '±11,794',\n",
       "       '149,900', '±5,682', '206,800', '±7,610', '97,700', '±5,435',\n",
       "       '130,500', '±7,981', '191,000', '±5,154', '113,300', '±17,608',\n",
       "       '105,000', '±7,545', '124,400', '±10,023', '209,900', '±7,520',\n",
       "       '133,000', '±7,102', '189,500', '±18,722', '247,500', '±4,622',\n",
       "       '385,900', '±16,160', '94,700', '±15,681', '232,900', '±5,271',\n",
       "       '227,300', '±5,050', '113,900', '±4,904', '131,600', '±7,706',\n",
       "       '228,300', '±7,193', '88,600', '±5,746', '148,500', '±10,691',\n",
       "       '132,600', '±8,344', '142,000', '±7,157', '138,800', '±2,991',\n",
       "       '255,600', '±14,064', '186,800', '±13,843', '155,800', '±6,947',\n",
       "       '157,400', '±5,517', '269,200', '±6,791', '173,800', '±4,202',\n",
       "       '178,400', '±17,095', '123,200', '±7,027', '124,700', '±12,173',\n",
       "       '123,600', '±4,807', '240,400', '±8,201', '258,300', '±20,215',\n",
       "       '573,500', '±13,048', '137,300', '±6,446', '125,000', '±9,143',\n",
       "       '135,900', '±7,563', '238,700', '±13,857', '195,300', '±13,748',\n",
       "       '177,800', '±7,948', '100,000', '±11,531', '214,500', '±6,680',\n",
       "       '181,300', '±15,573', '130,700', '±7,648', '229,400', '±15,063',\n",
       "       '101,500', '±8,522', '177,800', '±11,010', '137,500', '±7,132',\n",
       "       '137,200', '±8,699', '134,600', '±10,862', '165,200', '±8,539',\n",
       "       '175,700', '±7,824', '148,400', '±8,499', '204,300', '±15,365',\n",
       "       '131,600', '±7,286', '102,600', '±14,057', '393,000', '±14,811',\n",
       "       '274,200', '±5,296', '278,400', '±6,181', '140,800', '±3,826',\n",
       "       '95,400', '±7,598', '159,500', '±5,363', '134,700', '±11,877',\n",
       "       '394,700', '±18,991', '110,200', '±7,573', '141,300', '±8,931',\n",
       "       '161,700', '±6,221', '163,000', '±11,775', '132,800', '±9,344',\n",
       "       '225,600', '±5,469', '268,000', '±20,229', '184,700', '±5,244',\n",
       "       '155,500', '±7,439', '159,100', '±7,282', '144,100', '±11,600',\n",
       "       '198,400', '±16,667', '150,800', '±4,828', '147,600', '±2,851',\n",
       "       '173,400', '±7,712', '160,100', '±3,746', '155,200', '±5,261',\n",
       "       '167,600', '±3,602', '199,100', '±9,591', '121,700', '±9,639',\n",
       "       '186,700', '±9,194', '554,600', '±8,672', '148,300', '±21,755',\n",
       "       '122,300', '±14,354', '134,500', '±6,305', '183,600', '±11,116',\n",
       "       '126,700', '±12,073', '174,600', '±10,037', '206,100', '±20,597',\n",
       "       '162,400', '±8,761', '144,100', '±17,415', '219,800', '±6,256',\n",
       "       '150,600', '±14,275', '330,700', '±21,561', '106,500', '±16,909',\n",
       "       '359,900', '±8,764', '252,600', '±5,873', '109,500', '±6,379',\n",
       "       '317,600', '±19,448', '311,900', '±24,147', '145,800', '±3,629',\n",
       "       '128,800', '±18,299', '192,800', '±9,409', '124,600', '±6,442',\n",
       "       '169,800', '±6,558', '187,600', '±8,483', '176,300', '±10,661',\n",
       "       '293,700', '±10,788', '148,900', '±4,785', '136,500', '±14,097',\n",
       "       '163,900', '±6,343', '133,000', '±11,926', '149,900', '±14,047',\n",
       "       '472,200', '±16,070', '252,800', '±4,234', '168,200', '±8,405',\n",
       "       '164,300', '±3,800', '181,300', '±11,035', '409,600', '±9,515',\n",
       "       '111,400', '±10,382', '98,600', '±5,992', '89,500', '±7,524',\n",
       "       '186,900', '±7,583', '451,300', '±11,614', '228,100', '±4,670',\n",
       "       '139,800', '±6,837', '100,800', '±8,167', '134,900', '±11,556',\n",
       "       '564,000', '±10,097', '154,200', '±4,921', '261,500', '±10,516',\n",
       "       '170,200', '±5,253', '135,000', '±22,197', '113,800', '±12,940',\n",
       "       '244,200', '±5,607', '114,900', '±13,071', '162,300', '±5,644',\n",
       "       '146,700', '±3,864', '175,700', '±8,075', '80,600', '±5,745',\n",
       "       '166,200', '±6,961', '174,300', '±12,454', '294,800', '±4,853',\n",
       "       '130,600', '±5,499', '134,500', '±8,802', '133,900', '±6,705',\n",
       "       '359,300', '±8,329', '340,900', '±13,513', '307,700', '±10,051',\n",
       "       '128,400', '±10,408', '224,000', '±9,095', '84,100', '±3,989',\n",
       "       '222,900', '±22,275', '252,500', '±14,418', '120,800', '±10,309',\n",
       "       '397,800', '±10,599', '135,100', '±12,443', '157,400', '±5,186',\n",
       "       '206,900', '±9,040', '323,800', '±11,194', '171,800', '±4,575',\n",
       "       '119,200', '±8,475', '220,800', '±4,136', '202,500', '±5,095',\n",
       "       '138,800', '±15,112', '201,100', '±13,482', '98,600', '±21,095',\n",
       "       '147,600', '±11,045', '216,600', '±10,188', '87,700', '±16,306',\n",
       "       '162,500', '±8,393', '102,100', '±9,539', '278,000', '±7,837',\n",
       "       '150,100', '±3,238', '209,400', '±25,969', '139,600', '±7,282',\n",
       "       '114,000', '±10,403', '123,600', '±15,106', '108,900', '±12,811',\n",
       "       '154,500', '±8,051', '161,600', '±9,657', '756,200', '±28,300',\n",
       "       '146,500', '±8,616', '197,700', '±5,598', '118,400', '±11,342',\n",
       "       '125,700', '±5,940', '126,400', '±10,554', '233,100', '±11,844',\n",
       "       '172,700', '±14,813', '105,200', '±5,380', '100,600', '±15,116',\n",
       "       '472,300', '±19,702', '103,300', '±7,013', '92,200', '±8,685',\n",
       "       '157,600', '±6,878', '86,200', '±5,390', '174,300', '±11,389',\n",
       "       '146,400', '±6,905', '241,400', '±4,398', '129,500', '±11,535',\n",
       "       '177,000', '±9,486', '256,700', '±19,592', '137,700', '±11,033',\n",
       "       '109,700', '±6,149', '158,200', '±9,327', '129,400', '±7,920',\n",
       "       '248,700', '±5,948', '175,200', '±7,888', '160,200', '±5,949',\n",
       "       '331,600', '±12,167', '265,500', '±20,315', '117,000', '±7,914',\n",
       "       '198,200', '±13,418', '277,000', '±13,871', '181,700', '±10,098',\n",
       "       '162,400', '±3,722', '142,000', '±6,743', '420,900', '±5,561',\n",
       "       '458,100', '±17,720', '461,900', '±40,672', '184,200', '±13,625',\n",
       "       '115,800', '±6,701', '141,700', '±3,644', '200,900', '±13,628',\n",
       "       '129,300', '±7,609', '115,200', '±4,649', '126,000', '±7,759',\n",
       "       '148,200', '±8,679', '186,800', '±10,974', '125,200', '±6,734',\n",
       "       '192,500', '±8,213', '98,700', '±4,746', '173,700', '±9,581',\n",
       "       '193,000', '±6,783', '178,800', '±14,854', '174,400', '±19,257',\n",
       "       '95,600', '±7,356', '117,900', '±6,325', '241,200', '±33,480',\n",
       "       '159,100', '±7,501', '129,900', '±13,111', '153,000', '±10,651',\n",
       "       '149,600', '±6,224', '131,800', '±5,958', '255,000', '±7,049',\n",
       "       '90,500', '±6,351', '168,100', '±7,634', '113,200', '±5,009',\n",
       "       '165,900', '±5,384', '310,100', '±16,525', '158,800', '±8,108',\n",
       "       '243,700', '±11,103', '164,600', '±16,742', '165,300', '±11,614',\n",
       "       '155,300', '±8,949', '184,500', '±19,865', '153,100', '±12,000',\n",
       "       '174,700', '±16,514', '253,400', '±13,426', '128,100', '±7,303',\n",
       "       '130,500', '±12,765', '148,800', '±10,944', '100,500', '±9,382',\n",
       "       '428,800', '±20,520', '140,000', '±3,480', '318,700', '±11,898',\n",
       "       '164,000', '±13,763', '133,500', '±7,216', '172,300', '±9,295',\n",
       "       '274,400', '±12,620', '138,800', '±9,164', '150,800', '±12,522',\n",
       "       '274,500', '±12,336', '424,100', '±10,434', '163,800', '±4,917',\n",
       "       '169,500', '±6,919', '153,800', '±4,466', '110,600', '±9,884',\n",
       "       '110,200', '±17,053', '99,700', '±6,308', '140,200', '±10,329',\n",
       "       '335,700', '±12,202', '181,600', '±9,733', '240,300', '±21,669',\n",
       "       '135,800', '±3,440', '234,300', '±4,411', '280,500', '±5,699',\n",
       "       '204,600', '±12,091', '337,100', '±17,319', '688,100', '±7,487',\n",
       "       '198,200', '±10,286', '133,000', '±10,968', '310,900', '±7,659',\n",
       "       '108,200', '±17,713', '138,000', '±5,160', '203,100', '±15,361',\n",
       "       '105,000', '±13,904', '131,200', '±29,693', '238,700', '±5,241',\n",
       "       '126,700', '±19,676', '132,700', '±12,792', '121,600', '±5,464',\n",
       "       '155,500', '±4,606', '112,800', '±13,513', '160,100', '±4,835',\n",
       "       '441,100', '±7,149', '373,300', '±15,070', '180,400', '±10,814',\n",
       "       '150,500', '±5,278', '160,000', '±8,112', '221,300', '±13,467',\n",
       "       '101,900', '±19,897', '209,500', '±8,816', '183,800', '±5,802',\n",
       "       '109,800', '±12,685', '152,500', '±5,336', '269,700', '±10,251',\n",
       "       '109,200', '±11,172', '155,000', '±7,974', '115,200', '±19,775',\n",
       "       '143,600', '±7,379', '118,300'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.MEDIAN_VAL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-575fea2df78f>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  house['STATE'] = house['STATE'].apply(lambda x: str(x).strip())\n"
     ]
    }
   ],
   "source": [
    "# lets also strip whitespace from STATE for good measure\n",
    "house['STATE'] = house['STATE'].apply(lambda x: str(x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing margins of error from house\n",
    "house = house[~house.COUNTY.str.contains('MAR')]\n",
    "\n",
    "# removing alaska and hawaii\n",
    "house = house[~house.COUNTY.str.contains('AK')]\n",
    "house = house[~house.COUNTY.str.contains('HI')]\n",
    "\n",
    "house.head()\n",
    "\n",
    "# removing state and est from COUNTY, uppercasing\n",
    "house.COUNTY = house.COUNTY.apply(lambda x: str(x)[:-7].upper())\n",
    "\n",
    "# stripping whitespace from COUNTY\n",
    "house['COUNTY'] = house['COUNTY'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Missing States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "states = [\"\", \"\", \"\", \"\", \"\", \"\", \"CT\", \"\", \"\", \"\", \n",
    "          \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \n",
    "          \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \n",
    "          \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \n",
    "          \"\", \"\", \"\", \"\", \"VT\", \"\", \"\", \"WV\", \"\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hmm, seem to be only 48 states made it into the df... is that accurate???\n",
    "len(house['STATE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AZ', 'CA', 'GA', 'IL', 'IN', 'KS', 'MD', 'MS', 'NH', 'NM', 'NY',\n",
       "       'ND', 'RI', 'TX', 'VA', 'WA', 'CO', 'OH', 'MO', 'FL', 'IA', 'AL',\n",
       "       'LA', 'WV', 'OR', 'MI', 'SC', 'TN', 'KY', 'UT', 'PA', 'MN', 'NC',\n",
       "       'AR', 'MT', 'NJ', 'WY', 'MA', 'WI', 'OK', 'ID', 'NV', 'SD', 'NE',\n",
       "       'DE', 'ME'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's identify which states aren't included.\n",
    "house['STATE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we seem to be missing entries for CT, VT, and WV. Let's verify that these exist\n",
    "\n",
    "there seems to be no entries for VT. What? Oh well. There are for WV and CT. Where did they go when we created STATE column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating RUCC\n",
    "It's been difficult to find housing data for 1993, so I think I might just update the urban-rural continuum codes so that I can use housing prices that are more recent. I have 2013 codes [here](https://www.ers.usda.gov/data-products/rural-urban-continuum-codes.aspx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County_Name</th>\n",
       "      <th>Population_2010</th>\n",
       "      <th>RUCC_2013</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>AL</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>54,571</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Metro - Counties in metro areas of 250,000 to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>182,265</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Metro - Counties in metro areas of fewer than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>AL</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>27,457</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Nonmetro - Urban population of 2,500 to 19,999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>AL</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>22,915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Metro - Counties in metro areas of 1 million p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>AL</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>57,322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Metro - Counties in metro areas of 1 million p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS State     County_Name Population_2010  RUCC_2013  \\\n",
       "0  1001    AL  Autauga County          54,571        2.0   \n",
       "1  1003    AL  Baldwin County         182,265        3.0   \n",
       "2  1005    AL  Barbour County          27,457        6.0   \n",
       "3  1007    AL     Bibb County          22,915        1.0   \n",
       "4  1009    AL   Blount County          57,322        1.0   \n",
       "\n",
       "                                         Description  \n",
       "0  Metro - Counties in metro areas of 250,000 to ...  \n",
       "1  Metro - Counties in metro areas of fewer than ...  \n",
       "2  Nonmetro - Urban population of 2,500 to 19,999...  \n",
       "3  Metro - Counties in metro areas of 1 million p...  \n",
       "4  Metro - Counties in metro areas of 1 million p...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing RUC_2013\n",
    "ruc_13 = pd.read_csv('/Users/patricknorman/Documents/GitHub/metis-project-2/data/ruc_13/ruc_2013.csv')\n",
    "ruc_13.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>COMBINED_COUNTY_INDEX</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CENSUS_DIV</th>\n",
       "      <th>RUC_93</th>\n",
       "      <th>UIC_93</th>\n",
       "      <th>JAN_TEMP</th>\n",
       "      <th>JAN_SUN</th>\n",
       "      <th>JULY_TEMP</th>\n",
       "      <th>...</th>\n",
       "      <th>WATER</th>\n",
       "      <th>LN_WATER</th>\n",
       "      <th>JAN TEMP - Z</th>\n",
       "      <th>JAN SUN - Z</th>\n",
       "      <th>JUL TEMP - Z</th>\n",
       "      <th>JUL HUM - Z</th>\n",
       "      <th>TOPOG - Z</th>\n",
       "      <th>LN WATER  AREA - Z</th>\n",
       "      <th>NAT_AMENITY</th>\n",
       "      <th>RANK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>AL</td>\n",
       "      <td>AUTAUGA</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>130</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>4.944</td>\n",
       "      <td>1.20277</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.13206</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>0.26585</td>\n",
       "      <td>0.78</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>51.9</td>\n",
       "      <td>152</td>\n",
       "      <td>80.6</td>\n",
       "      <td>...</td>\n",
       "      <td>21.24</td>\n",
       "      <td>7.661</td>\n",
       "      <td>1.57504</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.36308</td>\n",
       "      <td>-1.09576</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.70428</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>AL</td>\n",
       "      <td>BARBOUR</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>48.8</td>\n",
       "      <td>152</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.378</td>\n",
       "      <td>1.31859</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>-0.01148</td>\n",
       "      <td>-0.89039</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.49570</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>1007</td>\n",
       "      <td>AL</td>\n",
       "      <td>BIBB</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>44.8</td>\n",
       "      <td>130</td>\n",
       "      <td>80.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.916</td>\n",
       "      <td>0.98768</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.30223</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>-0.27812</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>1009</td>\n",
       "      <td>AL</td>\n",
       "      <td>BLOUNT</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41.9</td>\n",
       "      <td>130</td>\n",
       "      <td>78.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4.336</td>\n",
       "      <td>0.74778</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>0.09330</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>-0.05592</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS  COMBINED_COUNTY_INDEX STATE   COUNTY  CENSUS_DIV  RUC_93  UIC_93  \\\n",
       "0  1001                   1001    AL  AUTAUGA           6       2       2   \n",
       "1  1003                   1003    AL  BALDWIN           6       2       2   \n",
       "2  1005                   1005    AL  BARBOUR           6       6       5   \n",
       "3  1007                   1007    AL     BIBB           6       6       6   \n",
       "4  1009                   1009    AL   BLOUNT           6       2       2   \n",
       "\n",
       "   JAN_TEMP  JAN_SUN  JULY_TEMP  ...  WATER  LN_WATER  JAN TEMP - Z  \\\n",
       "0      47.4      130       81.0  ...   1.40     4.944       1.20277   \n",
       "1      51.9      152       80.6  ...  21.24     7.661       1.57504   \n",
       "2      48.8      152       81.0  ...   2.17     5.378       1.31859   \n",
       "3      44.8      130       80.8  ...   0.50     3.916       0.98768   \n",
       "4      41.9      130       78.4  ...   0.76     4.336       0.74778   \n",
       "\n",
       "   JAN SUN - Z  JUL TEMP - Z  JUL HUM - Z  TOPOG - Z  LN WATER  AREA - Z  \\\n",
       "0     -0.64758      -0.13206     -0.68502    0.77701             0.26585   \n",
       "1      0.01482       0.36308     -1.09576   -0.73966             1.70428   \n",
       "2      0.01482      -0.01148     -0.89039   -0.73966             0.49570   \n",
       "3     -0.64758      -0.30223     -0.68502    0.77701            -0.27812   \n",
       "4     -0.64758       0.09330     -0.68502    0.77701            -0.05592   \n",
       "\n",
       "   NAT_AMENITY  RANK  \n",
       "0         0.78     4  \n",
       "1         1.82     4  \n",
       "2         0.19     4  \n",
       "3        -0.15     3  \n",
       "4         0.23     4  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>COMBINED_COUNTY_INDEX</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CENSUS_DIV</th>\n",
       "      <th>JAN_TEMP</th>\n",
       "      <th>JAN_SUN</th>\n",
       "      <th>JULY_TEMP</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>TOPO</th>\n",
       "      <th>...</th>\n",
       "      <th>LN_WATER</th>\n",
       "      <th>JAN TEMP - Z</th>\n",
       "      <th>JAN SUN - Z</th>\n",
       "      <th>JUL TEMP - Z</th>\n",
       "      <th>JUL HUM - Z</th>\n",
       "      <th>TOPOG - Z</th>\n",
       "      <th>LN WATER  AREA - Z</th>\n",
       "      <th>NAT_AMENITY</th>\n",
       "      <th>RANK</th>\n",
       "      <th>RUCC_2013</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>AL</td>\n",
       "      <td>AUTAUGA</td>\n",
       "      <td>6</td>\n",
       "      <td>47.4</td>\n",
       "      <td>130</td>\n",
       "      <td>81.0</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>4.944</td>\n",
       "      <td>1.20277</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.13206</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>0.26585</td>\n",
       "      <td>0.78</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>6</td>\n",
       "      <td>51.9</td>\n",
       "      <td>152</td>\n",
       "      <td>80.6</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>7.661</td>\n",
       "      <td>1.57504</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.36308</td>\n",
       "      <td>-1.09576</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.70428</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>AL</td>\n",
       "      <td>BARBOUR</td>\n",
       "      <td>6</td>\n",
       "      <td>48.8</td>\n",
       "      <td>152</td>\n",
       "      <td>81.0</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.378</td>\n",
       "      <td>1.31859</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>-0.01148</td>\n",
       "      <td>-0.89039</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.49570</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>1007</td>\n",
       "      <td>AL</td>\n",
       "      <td>BIBB</td>\n",
       "      <td>6</td>\n",
       "      <td>44.8</td>\n",
       "      <td>130</td>\n",
       "      <td>80.8</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>3.916</td>\n",
       "      <td>0.98768</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.30223</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>-0.27812</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>1009</td>\n",
       "      <td>AL</td>\n",
       "      <td>BLOUNT</td>\n",
       "      <td>6</td>\n",
       "      <td>41.9</td>\n",
       "      <td>130</td>\n",
       "      <td>78.4</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>4.336</td>\n",
       "      <td>0.74778</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>0.09330</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>-0.05592</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS  COMBINED_COUNTY_INDEX STATE   COUNTY  CENSUS_DIV  JAN_TEMP  JAN_SUN  \\\n",
       "0  1001                   1001    AL  AUTAUGA           6      47.4      130   \n",
       "1  1003                   1003    AL  BALDWIN           6      51.9      152   \n",
       "2  1005                   1005    AL  BARBOUR           6      48.8      152   \n",
       "3  1007                   1007    AL     BIBB           6      44.8      130   \n",
       "4  1009                   1009    AL   BLOUNT           6      41.9      130   \n",
       "\n",
       "   JULY_TEMP  HUMIDITY  TOPO  ...  LN_WATER  JAN TEMP - Z  JAN SUN - Z  \\\n",
       "0       81.0        66    14  ...     4.944       1.20277     -0.64758   \n",
       "1       80.6        72     4  ...     7.661       1.57504      0.01482   \n",
       "2       81.0        69     4  ...     5.378       1.31859      0.01482   \n",
       "3       80.8        66    14  ...     3.916       0.98768     -0.64758   \n",
       "4       78.4        66    14  ...     4.336       0.74778     -0.64758   \n",
       "\n",
       "   JUL TEMP - Z  JUL HUM - Z  TOPOG - Z  LN WATER  AREA - Z  NAT_AMENITY  \\\n",
       "0      -0.13206     -0.68502    0.77701             0.26585         0.78   \n",
       "1       0.36308     -1.09576   -0.73966             1.70428         1.82   \n",
       "2      -0.01148     -0.89039   -0.73966             0.49570         0.19   \n",
       "3      -0.30223     -0.68502    0.77701            -0.27812        -0.15   \n",
       "4       0.09330     -0.68502    0.77701            -0.05592         0.23   \n",
       "\n",
       "   RANK  RUCC_2013  \n",
       "0     4        2.0  \n",
       "1     4        3.0  \n",
       "2     4        6.0  \n",
       "3     3        1.0  \n",
       "4     4        1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging 2013 urban metric into nat\n",
    "nat_13 = pd.merge(nat,ruc_13['RUCC_2013'],\n",
    "                      left_on=nat['FIPS'],\n",
    "                      right_on=ruc_13['FIPS'])\n",
    "\n",
    "nat_13.drop('key_0',axis=1, inplace=True)\n",
    "nat_13.drop('RUC_93',axis=1, inplace=True)\n",
    "nat_13.drop('UIC_93',axis=1, inplace=True)\n",
    "\n",
    "nat_13.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining House and Natural Amenity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating columns to merge on\n",
    "house['MERGE'] = house['COUNTY'] + ' ' + house['STATE']\n",
    "\n",
    "nat_13['MERGE'] = nat['COUNTY'] + ' ' + nat['STATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_0</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>COMBINED_COUNTY_INDEX</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CENSUS_DIV</th>\n",
       "      <th>JAN_TEMP</th>\n",
       "      <th>JAN_SUN</th>\n",
       "      <th>JULY_TEMP</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>...</th>\n",
       "      <th>JAN SUN - Z</th>\n",
       "      <th>JUL TEMP - Z</th>\n",
       "      <th>JUL HUM - Z</th>\n",
       "      <th>TOPOG - Z</th>\n",
       "      <th>LN WATER  AREA - Z</th>\n",
       "      <th>NAT_AMENITY</th>\n",
       "      <th>RANK</th>\n",
       "      <th>RUCC_2013</th>\n",
       "      <th>MERGE</th>\n",
       "      <th>MEDIAN_VAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BALDWIN AL</td>\n",
       "      <td>1003</td>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>6</td>\n",
       "      <td>51.9</td>\n",
       "      <td>152</td>\n",
       "      <td>80.6</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.36308</td>\n",
       "      <td>-1.09576</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.70428</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>BALDWIN AL</td>\n",
       "      <td>176,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETOWAH AL</td>\n",
       "      <td>1055</td>\n",
       "      <td>1055</td>\n",
       "      <td>AL</td>\n",
       "      <td>ETOWAH</td>\n",
       "      <td>6</td>\n",
       "      <td>42.8</td>\n",
       "      <td>130</td>\n",
       "      <td>79.2</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.04428</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>0.58125</td>\n",
       "      <td>0.96</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ETOWAH AL</td>\n",
       "      <td>124,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOUSTON AL</td>\n",
       "      <td>1069</td>\n",
       "      <td>1069</td>\n",
       "      <td>AL</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>6</td>\n",
       "      <td>50.6</td>\n",
       "      <td>152</td>\n",
       "      <td>81.3</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.06289</td>\n",
       "      <td>-0.89039</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>-0.70651</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>HOUSTON AL</td>\n",
       "      <td>131,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JEFFERSON AL</td>\n",
       "      <td>1073</td>\n",
       "      <td>1073</td>\n",
       "      <td>AL</td>\n",
       "      <td>JEFFERSON</td>\n",
       "      <td>6</td>\n",
       "      <td>44.2</td>\n",
       "      <td>130</td>\n",
       "      <td>79.9</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.11192</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>0.08485</td>\n",
       "      <td>0.51</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>JEFFERSON AL</td>\n",
       "      <td>155,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAUDERDALE AL</td>\n",
       "      <td>1077</td>\n",
       "      <td>1077</td>\n",
       "      <td>AL</td>\n",
       "      <td>LAUDERDALE</td>\n",
       "      <td>6</td>\n",
       "      <td>41.3</td>\n",
       "      <td>130</td>\n",
       "      <td>80.1</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.41547</td>\n",
       "      <td>-0.61656</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>1.10611</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>LAUDERDALE AL</td>\n",
       "      <td>122,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>WALWORTH WI</td>\n",
       "      <td>55135</td>\n",
       "      <td>55135</td>\n",
       "      <td>WI</td>\n",
       "      <td>WAUPACA</td>\n",
       "      <td>3</td>\n",
       "      <td>16.2</td>\n",
       "      <td>137</td>\n",
       "      <td>71.0</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.43682</td>\n",
       "      <td>-0.13055</td>\n",
       "      <td>-0.20582</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.41532</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>WALWORTH WI</td>\n",
       "      <td>191,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>WAUKESHA WI</td>\n",
       "      <td>55141</td>\n",
       "      <td>55141</td>\n",
       "      <td>WI</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>137</td>\n",
       "      <td>69.8</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.43682</td>\n",
       "      <td>0.01984</td>\n",
       "      <td>-0.20582</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.46841</td>\n",
       "      <td>-2.44</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WAUKESHA WI</td>\n",
       "      <td>255,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>WINNEBAGO WI</td>\n",
       "      <td>56005</td>\n",
       "      <td>56005</td>\n",
       "      <td>WY</td>\n",
       "      <td>CAMPBELL</td>\n",
       "      <td>8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>158</td>\n",
       "      <td>70.7</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19547</td>\n",
       "      <td>0.42383</td>\n",
       "      <td>1.30024</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>-1.13236</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>WINNEBAGO WI</td>\n",
       "      <td>139,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>WOOD WI</td>\n",
       "      <td>56007</td>\n",
       "      <td>56007</td>\n",
       "      <td>WY</td>\n",
       "      <td>CARBON</td>\n",
       "      <td>8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>190</td>\n",
       "      <td>66.0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15896</td>\n",
       "      <td>1.68757</td>\n",
       "      <td>1.64252</td>\n",
       "      <td>1.83868</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>5.41</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>WOOD WI</td>\n",
       "      <td>120,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>LARAMIE WY</td>\n",
       "      <td>56029</td>\n",
       "      <td>56029</td>\n",
       "      <td>WY</td>\n",
       "      <td>PARK</td>\n",
       "      <td>8</td>\n",
       "      <td>24.4</td>\n",
       "      <td>167</td>\n",
       "      <td>69.4</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46645</td>\n",
       "      <td>1.00593</td>\n",
       "      <td>1.77944</td>\n",
       "      <td>1.83868</td>\n",
       "      <td>-0.43299</td>\n",
       "      <td>3.96</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>LARAMIE WY</td>\n",
       "      <td>208,600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             key_0   FIPS  COMBINED_COUNTY_INDEX STATE      COUNTY  \\\n",
       "0       BALDWIN AL   1003                   1003    AL     BALDWIN   \n",
       "1        ETOWAH AL   1055                   1055    AL      ETOWAH   \n",
       "2       HOUSTON AL   1069                   1069    AL     HOUSTON   \n",
       "3     JEFFERSON AL   1073                   1073    AL   JEFFERSON   \n",
       "4    LAUDERDALE AL   1077                   1077    AL  LAUDERDALE   \n",
       "..             ...    ...                    ...   ...         ...   \n",
       "439    WALWORTH WI  55135                  55135    WI     WAUPACA   \n",
       "440    WAUKESHA WI  55141                  55141    WI        WOOD   \n",
       "441   WINNEBAGO WI  56005                  56005    WY    CAMPBELL   \n",
       "442        WOOD WI  56007                  56007    WY      CARBON   \n",
       "443     LARAMIE WY  56029                  56029    WY        PARK   \n",
       "\n",
       "     CENSUS_DIV  JAN_TEMP  JAN_SUN  JULY_TEMP  HUMIDITY  ...  JAN SUN - Z  \\\n",
       "0             6      51.9      152       80.6        72  ...      0.01482   \n",
       "1             6      42.8      130       79.2        66  ...     -0.64758   \n",
       "2             6      50.6      152       81.3        69  ...      0.01482   \n",
       "3             6      44.2      130       79.9        66  ...     -0.64758   \n",
       "4             6      41.3      130       80.1        65  ...     -0.64758   \n",
       "..          ...       ...      ...        ...       ...  ...          ...   \n",
       "439           3      16.2      137       71.0        59  ...     -0.43682   \n",
       "440           3      14.2      137       69.8        59  ...     -0.43682   \n",
       "441           8      21.7      158       70.7        37  ...      0.19547   \n",
       "442           8      21.7      190       66.0        32  ...      1.15896   \n",
       "443           8      24.4      167       69.4        30  ...      0.46645   \n",
       "\n",
       "     JUL TEMP - Z  JUL HUM - Z  TOPOG - Z  LN WATER  AREA - Z  NAT_AMENITY  \\\n",
       "0         0.36308     -1.09576   -0.73966             1.70428         1.82   \n",
       "1        -0.04428     -0.68502    0.92868             0.58125         0.96   \n",
       "2         0.06289     -0.89039   -0.73966            -0.70651        -0.79   \n",
       "3        -0.11192     -0.68502    0.92868             0.08485         0.51   \n",
       "4        -0.41547     -0.61656    0.77701             1.10611         0.90   \n",
       "..            ...          ...        ...                 ...          ...   \n",
       "439      -0.13055     -0.20582   -0.73966             0.41532        -2.48   \n",
       "440       0.01984     -0.20582   -0.73966             0.46841        -2.44   \n",
       "441       0.42383      1.30024    0.92868            -1.13236         0.79   \n",
       "442       1.68757      1.64252    1.83868             0.00191         5.41   \n",
       "443       1.00593      1.77944    1.83868            -0.43299         3.96   \n",
       "\n",
       "     RANK  RUCC_2013          MERGE  MEDIAN_VAL  \n",
       "0       4        3.0     BALDWIN AL     176,300  \n",
       "1       4        3.0      ETOWAH AL     124,400  \n",
       "2       3        3.0     HOUSTON AL     131,800  \n",
       "3       4        1.0   JEFFERSON AL     155,800  \n",
       "4       4        3.0  LAUDERDALE AL     122,100  \n",
       "..    ...        ...            ...         ...  \n",
       "439     2        6.0    WALWORTH WI     191,900  \n",
       "440     2        4.0    WAUKESHA WI     255,000  \n",
       "441     4        5.0   WINNEBAGO WI     139,800  \n",
       "442     6        7.0        WOOD WI     120,800  \n",
       "443     5        7.0     LARAMIE WY     208,600  \n",
       "\n",
       "[444 rows x 24 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to merge the house and nat dfs\n",
    "\n",
    "combo = pd.merge(nat_13,house['MEDIAN_VAL'],\n",
    "                left_on=nat_13['MERGE'],\n",
    "                right_on=house['MERGE'])\n",
    "\n",
    "combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up combo df\n",
    "combo.drop('key_0',axis=1,inplace=True)\n",
    "combo.drop('COMBINED_COUNTY_INDEX',axis=1,inplace=True)\n",
    "combo.drop('MERGE',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CENSUS_DIV</th>\n",
       "      <th>JAN_TEMP</th>\n",
       "      <th>JAN_SUN</th>\n",
       "      <th>JULY_TEMP</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>TOPO</th>\n",
       "      <th>WATER</th>\n",
       "      <th>...</th>\n",
       "      <th>JAN TEMP - Z</th>\n",
       "      <th>JAN SUN - Z</th>\n",
       "      <th>JUL TEMP - Z</th>\n",
       "      <th>JUL HUM - Z</th>\n",
       "      <th>TOPOG - Z</th>\n",
       "      <th>LN WATER  AREA - Z</th>\n",
       "      <th>NAT_AMENITY</th>\n",
       "      <th>RANK</th>\n",
       "      <th>RUCC_2013</th>\n",
       "      <th>MEDIAN_VAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1083</td>\n",
       "      <td>AL</td>\n",
       "      <td>LIMESTONE</td>\n",
       "      <td>6</td>\n",
       "      <td>41.2</td>\n",
       "      <td>130</td>\n",
       "      <td>79.2</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>6.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68987</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.18209</td>\n",
       "      <td>-0.61656</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.07166</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>158,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1089</td>\n",
       "      <td>AL</td>\n",
       "      <td>MADISON</td>\n",
       "      <td>6</td>\n",
       "      <td>40.9</td>\n",
       "      <td>130</td>\n",
       "      <td>79.5</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66505</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.28860</td>\n",
       "      <td>-0.61656</td>\n",
       "      <td>-0.43633</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1095</td>\n",
       "      <td>AL</td>\n",
       "      <td>MARSHALL</td>\n",
       "      <td>6</td>\n",
       "      <td>40.9</td>\n",
       "      <td>130</td>\n",
       "      <td>77.6</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66505</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>0.22228</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>-0.43633</td>\n",
       "      <td>1.24995</td>\n",
       "      <td>0.37</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1097</td>\n",
       "      <td>AL</td>\n",
       "      <td>MOBILE</td>\n",
       "      <td>6</td>\n",
       "      <td>51.2</td>\n",
       "      <td>152</td>\n",
       "      <td>81.6</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>24.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1.51713</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.03390</td>\n",
       "      <td>-1.09576</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.79023</td>\n",
       "      <td>1.52</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>131,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1117</td>\n",
       "      <td>AL</td>\n",
       "      <td>SHELBY</td>\n",
       "      <td>6</td>\n",
       "      <td>45.0</td>\n",
       "      <td>130</td>\n",
       "      <td>79.3</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>1.81</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00423</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>0.11832</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>0.40169</td>\n",
       "      <td>1.12</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199,100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS STATE     COUNTY  CENSUS_DIV  JAN_TEMP  JAN_SUN  JULY_TEMP  HUMIDITY  \\\n",
       "5  1083    AL  LIMESTONE           6      41.2      130       79.2        65   \n",
       "6  1089    AL    MADISON           6      40.9      130       79.5        65   \n",
       "7  1095    AL   MARSHALL           6      40.9      130       77.6        66   \n",
       "8  1097    AL     MOBILE           6      51.2      152       81.6        72   \n",
       "9  1117    AL     SHELBY           6      45.0      130       79.3        66   \n",
       "\n",
       "   TOPO  WATER  ...  JAN TEMP - Z  JAN SUN - Z  JUL TEMP - Z  JUL HUM - Z  \\\n",
       "5     4   6.43  ...       0.68987     -0.64758      -0.18209     -0.61656   \n",
       "6     6   0.98  ...       0.66505     -0.64758      -0.28860     -0.61656   \n",
       "7     6   9.00  ...       0.66505     -0.64758       0.22228     -0.68502   \n",
       "8     4  24.98  ...       1.51713      0.01482       0.03390     -1.09576   \n",
       "9    15   1.81  ...       1.00423     -0.64758       0.11832     -0.68502   \n",
       "\n",
       "   TOPOG - Z  LN WATER  AREA - Z  NAT_AMENITY  RANK  RUCC_2013  MEDIAN_VAL  \n",
       "5   -0.73966             1.07166        -0.42     3        2.0     158,100  \n",
       "6   -0.43633             0.07356        -1.25     3        2.0     163,900  \n",
       "7   -0.43633             1.24995         0.37     4        4.0     130,500  \n",
       "8   -0.73966             1.79023         1.52     4        2.0     131,600  \n",
       "9    0.92868             0.40169         1.12     4        1.0     199,100  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Population Density\n",
    "I had an idea on how to incorporate scraped data. Maybe I could scrape population density per county off of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tools\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to locate the target tag in one county, then apply it generally\n",
    "\n",
    "name = 'YUMA'\n",
    "state = 'AZ'\n",
    "yuma_url = 'https://en.wikipedia.org/wiki/Yuma,_Arizona'\n",
    "\n",
    "req = requests.get(yuma_url)\n",
    "soup = BeautifulSoup(req.text)\n",
    "\n",
    "bowl = soup.find_all(class_='mergedrow')\n",
    "#bowl_df = pd.DataFrame(bowl)\n",
    "\n",
    "#target = bowl_df[0][10]\n",
    "\n",
    "#result = np.zeros((0,3))\n",
    "#new_row = [name,state,target]\n",
    "\n",
    "#np.vstack((result,new_row))\n",
    "\n",
    "#print(result)\n",
    "headers = soup.find_all('th', scope='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['814.70/sq\\xa0mi (31.80/km2)']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = []\n",
    "\n",
    "for header in headers:\n",
    "    if 'Density' in header.text:\n",
    "        bucket.append(header.find_next().text)\n",
    "        \n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countyScrape(tuplist):\n",
    "    '''\n",
    "    returns the population density of a county based on wikipedia page\n",
    "    \n",
    "    tuplist: list of tuples in form ('County','State')\n",
    "    \n",
    "    returns a list of densities, along with a print statement going over\n",
    "    the number of errors, and the number of entries that seem to be related\n",
    "    to population density. \n",
    "    '''\n",
    "    densities = []\n",
    "    wins = 0\n",
    "    failures = 0\n",
    "    \n",
    "    for i in range(len(tuplist)):\n",
    "        name = tuplist[i][0]\n",
    "        state = tuplist[i][1]\n",
    "\n",
    "        url = f'https://en.wikipedia.org/wiki/{name}_County,_{state}'\n",
    "        print('scraping', url)\n",
    "        response = requests.get(url)\n",
    "        if int(response.status_code) == 404:\n",
    "            return 'error at index', i\n",
    "        \n",
    "        print('status code: ',response.status_code, 'for', name)\n",
    "\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        \n",
    "        bowl = soup.find_all('th',scope='row')\n",
    "        #print('bowl created')\n",
    "        \n",
    "        for header in bowl:\n",
    "            if 'Density' in header.text:\n",
    "                densities.append(header.find_next().text)\n",
    "        \n",
    "        print('pulling', densities[-1])\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateConvert(state_abb):\n",
    "    converter = {\"AL\":\"Alabama\",\"AK\":\"Alaska\",\"AZ\":\"Arizona\",\n",
    "                 \"AR\":\"Arkansas\",\"CA\":\"California\",\"CO\":\"Colorado\",\n",
    "                 \"CT\":\"Connecticut\",\"DE\":\"Delaware\",\"FL\":\"Florida\",\n",
    "                 \"GA\":\"Georgia\",\"HI\":\"Hawaii\",\"ID\":\"Idaho\",\"IL\":\"Illinois\",\n",
    "                 \"IN\":\"Indiana\",\"IA\":\"Iowa\",\"KS\":\"Kansas\",\"KY\":\"Kentucky\",\n",
    "                 \"LA\":\"Louisiana\",\"ME\":\"Maine\",\"MD\":\"Maryland\",\n",
    "                 \"MA\":\"Massachusetts\",\"MI\":\"Michigan\",\"MN\":\"Minnesota\",\n",
    "                 \"MS\":\"Mississippi\",\"MO\":\"Missouri\",\"MT\":\"Montana\",\n",
    "                 \"NE\":\"Nebraska\",\"NV\":\"Nevada\",\"NH\":\"New Hampshire\",\n",
    "                 \"NJ\":\"New Jersey\",\"NM\":\"New Mexico\",\"NY\":\"New York\",\n",
    "                 \"NC\":\"North Carolina\",\"ND\":\"North Dakota\",\"OH\":\"Ohio\",\n",
    "                 \"OK\":\"Oklahoma\",\"OR\":\"Oregon\",\"PA\":\"Pennsylvania\",\n",
    "                 \"RI\":\"Rhode Island\",\"SC\":\"South Carolina\",\n",
    "                 \"SD\":\"South Dakota\",\"TN\":\"Tennessee\",\"TX\":\"Texas\",\n",
    "                 \"UT\":\"Utah\",\"VT\":\"Vermont\",\"VA\":\"Virginia\",\n",
    "                 \"WA\":\"Washington\",\"WV\":\"West Virginia\",\"WI\":\"Wisconsin\",\n",
    "                 \"WY\":\"Wyoming\"}\n",
    "    \n",
    "    return converter[state_abb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists from combo df\n",
    "county_list = list(combo['COUNTY'])\n",
    "state_list = list(combo['STATE'])\n",
    "\n",
    "# formatting county_list\n",
    "for i in range(len(county_list)):\n",
    "    county_list[i] = county_list[i].title()\n",
    "    \n",
    "for i in range(len(state_list)):\n",
    "    state_list[i] = stateConvert(state_list[i])\n",
    "    \n",
    "# zipping lists together\n",
    "locator = list(zip(county_list,state_list))\n",
    "\n",
    "# turning list of tuples into array for vectorization / making life easier\n",
    "loc_arr = np.array(locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_arr[74] = ['Chattahoochee','Georgia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chattahoochee', 'Georgia'], dtype='<U14')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_arr[74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing misspellings, other errors\n",
    "loc_arr[74] = ('Chattahoochee','Georgia')\n",
    "loc_arr[319] = ('McKean', 'Pennsylvania')\n",
    "loc_arr[377] = ('McCulloch', 'Texas')\n",
    "loc_arr[409] = ('Bedford', 'Virginia')\n",
    "loc_arr[427] = ['McDowell','West_Virginia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_arr = np.core.defchararray.replace(loc_arr, ' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping https://en.wikipedia.org/wiki/Campbell_County,_Wyoming\n",
      "status code:  200 for Campbell\n",
      "pulling 9.6/sq mi (3.7/km2)\n",
      "\n",
      "\n",
      "\n",
      "scraping https://en.wikipedia.org/wiki/Carbon_County,_Wyoming\n",
      "status code:  200 for Carbon\n",
      "pulling 2.0/sq mi (0.77/km2)\n",
      "\n",
      "\n",
      "\n",
      "scraping https://en.wikipedia.org/wiki/Park_County,_Wyoming\n",
      "status code:  200 for Park\n",
      "pulling 4.0/sq mi (1.6/km2)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scraping pop density, compiling it in a list of str\n",
    "densities = countyScrape(loc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list to df, adding it to the combo df for FINAL (!) df for \n",
    "# analysis!!!\n",
    "\n",
    "density_df = pd.DataFrame(densities, columns= ['DENSITY'])\n",
    "\n",
    "df = pd.concat([combo,density_df],axis=1)\n",
    "\n",
    "    # removing gunk fron density column\n",
    "df['DENSITY'] = df['DENSITY'].apply(lambda x: x[:x.index('/')])\n",
    "\n",
    "    # stripping commas\n",
    "df['DENSITY'] = df['DENSITY'].apply(lambda x: x.replace(',',''))\n",
    "\n",
    "    # making densities numeric\n",
    "df['DENSITY'] = df['DENSITY'].apply(lambda x: float(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh, I almost forgot to make sure the MEDIAN VAL column is numerical, lol. \n",
    "df['MEDIAN_VAL'] = df['MEDIAN_VAL'].apply(lambda x: float(x.replace(',','')))\n",
    "type(df['MEDIAN_VAL'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('/Users/patricknorman/Documents/python/nat.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED ALERT!!!\n",
    "The median home price per county has no actual variance. I have to change my target if I want to have any results! My plan is to find median home price by Metro Statistical Area instead. Hopefully that will have some variance to work with. I have a median home sale price for single family homes by MSA. [Data](https://www.nar.realtor/research-and-statistics/housing-statistics/metropolitan-median-area-prices-and-affordability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CENSUS_DIV</th>\n",
       "      <th>JAN_TEMP</th>\n",
       "      <th>JAN_SUN</th>\n",
       "      <th>JULY_TEMP</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>TOPO</th>\n",
       "      <th>WATER</th>\n",
       "      <th>...</th>\n",
       "      <th>JAN TEMP - Z</th>\n",
       "      <th>JAN SUN - Z</th>\n",
       "      <th>JUL TEMP - Z</th>\n",
       "      <th>JUL HUM - Z</th>\n",
       "      <th>TOPOG - Z</th>\n",
       "      <th>LN WATER  AREA - Z</th>\n",
       "      <th>NAT_AMENITY</th>\n",
       "      <th>RANK</th>\n",
       "      <th>RUCC_2013</th>\n",
       "      <th>DENSITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>6</td>\n",
       "      <td>51.9</td>\n",
       "      <td>152</td>\n",
       "      <td>80.6</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>21.24</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57504</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.36308</td>\n",
       "      <td>-1.09576</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>1.70428</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1055</td>\n",
       "      <td>AL</td>\n",
       "      <td>ETOWAH</td>\n",
       "      <td>6</td>\n",
       "      <td>42.8</td>\n",
       "      <td>130</td>\n",
       "      <td>79.2</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>2.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.82223</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.04428</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>0.58125</td>\n",
       "      <td>0.96</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1069</td>\n",
       "      <td>AL</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>6</td>\n",
       "      <td>50.6</td>\n",
       "      <td>152</td>\n",
       "      <td>81.3</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.46749</td>\n",
       "      <td>0.01482</td>\n",
       "      <td>0.06289</td>\n",
       "      <td>-0.89039</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>-0.70651</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>AL</td>\n",
       "      <td>JEFFERSON</td>\n",
       "      <td>6</td>\n",
       "      <td>44.2</td>\n",
       "      <td>130</td>\n",
       "      <td>79.9</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93805</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.11192</td>\n",
       "      <td>-0.68502</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>0.08485</td>\n",
       "      <td>0.51</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1077</td>\n",
       "      <td>AL</td>\n",
       "      <td>LAUDERDALE</td>\n",
       "      <td>6</td>\n",
       "      <td>41.3</td>\n",
       "      <td>130</td>\n",
       "      <td>80.1</td>\n",
       "      <td>65</td>\n",
       "      <td>14</td>\n",
       "      <td>6.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69815</td>\n",
       "      <td>-0.64758</td>\n",
       "      <td>-0.41547</td>\n",
       "      <td>-0.61656</td>\n",
       "      <td>0.77701</td>\n",
       "      <td>1.10611</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>55135</td>\n",
       "      <td>WI</td>\n",
       "      <td>WAUPACA</td>\n",
       "      <td>3</td>\n",
       "      <td>16.2</td>\n",
       "      <td>137</td>\n",
       "      <td>71.0</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>1.86</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.37827</td>\n",
       "      <td>-0.43682</td>\n",
       "      <td>-0.13055</td>\n",
       "      <td>-0.20582</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.41532</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>55141</td>\n",
       "      <td>WI</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>137</td>\n",
       "      <td>69.8</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>2.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.54372</td>\n",
       "      <td>-0.43682</td>\n",
       "      <td>0.01984</td>\n",
       "      <td>-0.20582</td>\n",
       "      <td>-0.73966</td>\n",
       "      <td>0.46841</td>\n",
       "      <td>-2.44</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>56005</td>\n",
       "      <td>WY</td>\n",
       "      <td>CAMPBELL</td>\n",
       "      <td>8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>158</td>\n",
       "      <td>70.7</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.92328</td>\n",
       "      <td>0.19547</td>\n",
       "      <td>0.42383</td>\n",
       "      <td>1.30024</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>-1.13236</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>56007</td>\n",
       "      <td>WY</td>\n",
       "      <td>CARBON</td>\n",
       "      <td>8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>190</td>\n",
       "      <td>66.0</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>0.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.92328</td>\n",
       "      <td>1.15896</td>\n",
       "      <td>1.68757</td>\n",
       "      <td>1.64252</td>\n",
       "      <td>1.83868</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>5.41</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>56029</td>\n",
       "      <td>WY</td>\n",
       "      <td>PARK</td>\n",
       "      <td>8</td>\n",
       "      <td>24.4</td>\n",
       "      <td>167</td>\n",
       "      <td>69.4</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.69992</td>\n",
       "      <td>0.46645</td>\n",
       "      <td>1.00593</td>\n",
       "      <td>1.77944</td>\n",
       "      <td>1.83868</td>\n",
       "      <td>-0.43299</td>\n",
       "      <td>3.96</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS STATE      COUNTY  CENSUS_DIV  JAN_TEMP  JAN_SUN  JULY_TEMP  \\\n",
       "0     1003    AL     BALDWIN           6      51.9      152       80.6   \n",
       "1     1055    AL      ETOWAH           6      42.8      130       79.2   \n",
       "2     1069    AL     HOUSTON           6      50.6      152       81.3   \n",
       "3     1073    AL   JEFFERSON           6      44.2      130       79.9   \n",
       "4     1077    AL  LAUDERDALE           6      41.3      130       80.1   \n",
       "..     ...   ...         ...         ...       ...      ...        ...   \n",
       "439  55135    WI     WAUPACA           3      16.2      137       71.0   \n",
       "440  55141    WI        WOOD           3      14.2      137       69.8   \n",
       "441  56005    WY    CAMPBELL           8      21.7      158       70.7   \n",
       "442  56007    WY      CARBON           8      21.7      190       66.0   \n",
       "443  56029    WY        PARK           8      24.4      167       69.4   \n",
       "\n",
       "     HUMIDITY  TOPO  WATER  ...  JAN TEMP - Z  JAN SUN - Z  JUL TEMP - Z  \\\n",
       "0          72     4  21.24  ...       1.57504      0.01482       0.36308   \n",
       "1          66    15   2.55  ...       0.82223     -0.64758      -0.04428   \n",
       "2          69     4   0.22  ...       1.46749      0.01482       0.06289   \n",
       "3          66    15   1.00  ...       0.93805     -0.64758      -0.11192   \n",
       "4          65    14   6.86  ...       0.69815     -0.64758      -0.41547   \n",
       "..        ...   ...    ...  ...           ...          ...           ...   \n",
       "439        59     4   1.86  ...      -1.37827     -0.43682      -0.13055   \n",
       "440        59     4   2.06  ...      -1.54372     -0.43682       0.01984   \n",
       "441        37    15   0.10  ...      -0.92328      0.19547       0.42383   \n",
       "442        32    21   0.85  ...      -0.92328      1.15896       1.68757   \n",
       "443        30    21   0.37  ...      -0.69992      0.46645       1.00593   \n",
       "\n",
       "     JUL HUM - Z  TOPOG - Z  LN WATER  AREA - Z  NAT_AMENITY  RANK  RUCC_2013  \\\n",
       "0       -1.09576   -0.73966             1.70428         1.82     4        3.0   \n",
       "1       -0.68502    0.92868             0.58125         0.96     4        3.0   \n",
       "2       -0.89039   -0.73966            -0.70651        -0.79     3        3.0   \n",
       "3       -0.68502    0.92868             0.08485         0.51     4        1.0   \n",
       "4       -0.61656    0.77701             1.10611         0.90     4        3.0   \n",
       "..           ...        ...                 ...          ...   ...        ...   \n",
       "439     -0.20582   -0.73966             0.41532        -2.48     2        6.0   \n",
       "440     -0.20582   -0.73966             0.46841        -2.44     2        4.0   \n",
       "441      1.30024    0.92868            -1.13236         0.79     4        5.0   \n",
       "442      1.64252    1.83868             0.00191         5.41     6        7.0   \n",
       "443      1.77944    1.83868            -0.43299         3.96     5        7.0   \n",
       "\n",
       "     DENSITY  \n",
       "0      137.0  \n",
       "1      190.0  \n",
       "2      170.0  \n",
       "3      590.0  \n",
       "4      130.0  \n",
       "..       ...  \n",
       "439     69.0  \n",
       "440     92.0  \n",
       "441      9.6  \n",
       "442      2.0  \n",
       "443      4.0  \n",
       "\n",
       "[444 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the old MEDIAN_VAL by county\n",
    "df.drop(columns='MEDIAN_VAL',inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_housing = pd.read_csv('/Users/patricknorman/Documents/GitHub/metis-project-2/data/msa_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_housing.rename(columns={'2019':'MEDIAN_VAL'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-60fbc457451a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# checking for enough variation (fingers crossed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_housing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MEDIAN_VAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# checking for enough variation (fingers crossed)\n",
    "plt.hist(msa_housing['MEDIAN_VAL']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msa_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like something I *might* be able to work with. \n",
    "\n",
    "Now I need to match CBSA codes to FIPS codes to get the county-level data in there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing, cleaning up CBSA_FIPS\n",
    "CBSA_FIPS = pd.read_csv('https://data.nber.org/cbsa-csa-fips-county-crosswalk/cbsa2fipsxw.csv')\n",
    "\n",
    "CBSA_FIPS.drop(columns=['metrodivisioncode', 'cbsatitle','metropolitanmicropolitanstatis','metropolitandivisiontitle','csatitle', 'csacode'], inplace=True)\n",
    "\n",
    "CBSA_FIPS.rename(columns={'cbsacode':'CBSA', 'countycountyequivalent':'COUNTY', 'statename':'STATE'},inplace=True)\n",
    "\n",
    "# removing some regions we don't want\n",
    "CBSA_FIPS = CBSA_FIPS[CBSA_FIPS['STATE'] != 'Puerto Rico']\n",
    "CBSA_FIPS = CBSA_FIPS[CBSA_FIPS['STATE'] != 'District of Columbia']\n",
    "CBSA_FIPS = CBSA_FIPS[CBSA_FIPS['STATE'] != 'Alaska']\n",
    "CBSA_FIPS = CBSA_FIPS[CBSA_FIPS['STATE'] != 'Hawaii']\n",
    "\n",
    "CBSA_FIPS.dropna()\n",
    "\n",
    "#CBSA_FIPS.statename.value_counts()\n",
    "#CBSA_FIPS.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBSA_FIPS = CBSA_FIPS[CBSA_FIPS['fipsstatecode'].notna()]\n",
    "\n",
    "CBSA_FIPS = CBSA_FIPS[CBSA_FIPS['fipscountycode'].notna()]\n",
    "\n",
    "CBSA_FIPS.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating combined FIPS column\n",
    "CBSA_FIPS['fipsstatecode'] = CBSA_FIPS['fipsstatecode'].apply(lambda x: int(x))\n",
    "CBSA_FIPS['fipscountycode'] = CBSA_FIPS['fipscountycode'].apply(lambda x: int(x))\n",
    "CBSA_FIPS['CBSA'] = CBSA_FIPS['CBSA'].apply(lambda x: int(x))\n",
    "\n",
    "CBSA_FIPS['FIPS'] = 1000*CBSA_FIPS['fipsstatecode'] + CBSA_FIPS['fipscountycode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBSA_FIPS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairing CBSA datapoints with the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, CBSA_FIPS[['FIPS','CBSA']], on='FIPS')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've paired the msa codes with the county codes, I need to get the housing costs into this combined dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,msa_housing[['CBSA','MEDIAN_VAL']], on='CBSA')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Finally I have all the data I need to perform a liner regression. \n",
    "\n",
    "0. Data Preparation for Regression\n",
    "1. EDA\n",
    "2. Test-Validate Split\n",
    "3. Train candidate models\n",
    "    1. Ridge w various alphas\n",
    "    2. LASSO w various alphas\n",
    "4. Validate candidates, choose final model and alpha parameter\n",
    "5. Train-Test Split using final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Regression\n",
    "I need to make sure that all my features are numerical, or at least ordinal. All are numerical except FIPS, CENSUS_DIV, TOPO, and RUCC_2013. Out of those, all are ordinal categories except CENSUS_DIV. \n",
    "I need to create dummy variables for the categorical feature, CENSUS_DIV. They're integers, but I still need to bust them into their own columns. I'm not sure how to approach this exact situation, so I'm actually going to make them string categories, and then use the process that I'm more familiar with for dealing with categories like that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting CENSUS_DIV to str, adding 'region' preceding value\n",
    "df['CENSUS_DIV'] = df['CENSUS_DIV'].apply(lambda x: 'region '+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_x = df.loc[:, ['CENSUS_DIV']]\n",
    "cat_y = df['MEDIAN_VAL']\n",
    "#cat_x.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided into 9 census regions that are encapsulated in CENSUS_DIV. So I should be able to create 8 new features to encapsulate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "ohe.fit(cat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming ohe_x, putting cat feature into a df\n",
    "ohe_x = ohe.transform(cat_x)\n",
    "columns = ohe.get_feature_names(['CENSUS_DIV'])\n",
    "ohe_x_df = pd.DataFrame(ohe_x, columns=columns, index=cat_x.index)\n",
    "# ohe_x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recombining the categorical and numeric variables\n",
    "df = pd.concat([df, ohe_x_df],axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.heatmap(df.corr(), cmap=\"seismic\", vmin=-1, vmax=1, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a first glance, it seems that the things that correlate the most strongly with MEDIAN_VAL are being in census region 9, the z-score for July temperature, and the natural amenity score, which is interesting... I expected the RUCC_2013 scores to be more correlated, but the relationship isn't that strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['MEDIAN_VAL'], df['NAT_AMENITY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-validate-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# separating target (y) from features (x)\n",
    "x, y = num_df.drop('MEDIAN_VAL',axis=1), df['MEDIAN_VAL']\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(x, y, test_size=.2, random_state=10) #hold out 20% of the data for final testing\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y, test_size = .25, random_state = 6)\n",
    "\n",
    "#this helps with the way kf will generate indices below\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard-Scaling my Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross-Validation to determine alpha and model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "x, y = num_df.drop('MEDIAN_VAL',axis=1), df['MEDIAN_VAL']\n",
    "x, x_test, y, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "x,y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression():\n",
    "    '''\n",
    "    regression(columns)\n",
    "    This function takes the columns that we want to create a regression of housing price based on, and\n",
    "    returns a plot of the mean absolute error for various regularization strengths, for both a LASSO \n",
    "    and Ridge regression.\n",
    "    '''\n",
    "    # split the data\n",
    "    x, y = num_df.drop('MEDIAN_VAL',axis=1), df['MEDIAN_VAL']\n",
    "    x, x_test, y, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "    x,y = np.array(x), np.array(y)\n",
    "    \n",
    "    kf = KFold(n_splits = 5, shuffle = True, random_state = 51)\n",
    "\n",
    "    # create array of regularization strengths\n",
    "    alphalist = 10**np.linspace(0,4,100)\n",
    "\n",
    "    # creating \n",
    "    ridge_val_err = np.zeros(len(alphalist))\n",
    "    lasso_val_err = np.zeros(len(alphalist))   \n",
    "\n",
    "    ridge_err_list = []\n",
    "    lasso_err_list = []\n",
    "\n",
    "    # looping through alpha values\n",
    "    for i,a in enumerate(alphalist):\n",
    "\n",
    "        ridge = Ridge(alpha=a)\n",
    "        lasso = Lasso(alpha=a)\n",
    "\n",
    "        ridge_err = []\n",
    "        lasso_err = []\n",
    "\n",
    "        # looping through k-folds\n",
    "        for train_index, val_index in kf.split(x,y):\n",
    "\n",
    "            # creating train / validation split\n",
    "            x_train, y_train = x[train_index], y[train_index]\n",
    "            x_val, y_val = x[val_index], y[val_index]  \n",
    "\n",
    "            #scaling features\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_val = scaler.transform(x_val)\n",
    "\n",
    "            # training models\n",
    "            ridge.fit(x_train,y_train)\n",
    "            lasso.fit(x_train, y_train)\n",
    "\n",
    "            # making predictions\n",
    "            ridge_pred = ridge.predict(x_val)\n",
    "            lasso_pred = lasso.predict(x_val)\n",
    "\n",
    "            # collecting results for the fold\n",
    "            ridge_err.append(r2_score(y_val, ridge_pred))\n",
    "            lasso_err.append(r2_score(y_val, lasso_pred))\n",
    "\n",
    "        # averaging errors in each fold\n",
    "\n",
    "        ridge_mean_err = np.mean(ridge_err)\n",
    "        lasso_mean_err = np.mean(lasso_err)\n",
    "\n",
    "        ridge_err_list.append(ridge_mean_err)\n",
    "        lasso_err_list.append(lasso_mean_err)\n",
    "\n",
    "    plt.plot(np.log10(alphalist), ridge_err_list, label = 'Ridge');\n",
    "    plt.plot(np.log10(alphalist), lasso_err_list, label = 'LASSO');\n",
    "    plt.title('r2 based on regularizations strength');\n",
    "    plt.legend();\n",
    "    plt.xlabel('Regularization strength (log scale)');\n",
    "    plt.ylabel('r2');\n",
    "\n",
    "    print('\\n The r2 for ridge was', np.max(ridge_err_list))\n",
    "    print('The r2 for LASSO was', np.max(lasso_err_list))\n",
    "    \n",
    "    plt.savefig('initial_reg_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.linear_model import LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Env factors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x = df[['JAN_TEMP','JAN_SUN','JULY_TEMP','HUMIDITY','TOPO','WATER', 'LN_WATER', 'JAN TEMP - Z',\n",
    "       'JAN SUN - Z', 'JUL TEMP - Z', 'JUL HUM - Z', 'TOPOG - Z',\n",
    "       'LN WATER  AREA - Z', 'NAT_AMENITY', 'RANK']]\n",
    "\n",
    "y = df['MEDIAN_VAL']\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "x,y = np.array(x), np.array(y)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "alphavec = np.linspace(2,50,100) # I ran this test a few times to find the best range for alphas\n",
    "\n",
    "# performing regression\n",
    "lasso = LassoCV(alphas = alphavec, cv= 5)\n",
    "ridge = RidgeCV(alphas = alphavec, cv= 5)\n",
    "\n",
    "lasso.fit(x,y)\n",
    "ridge.fit(x,y)\n",
    "\n",
    "lasso_pred = lasso.predict(x)\n",
    "ridge_pred = ridge.predict(x)\n",
    "\n",
    "print('Best LASSO alpha:     ', round(lasso.alpha_, 2))\n",
    "print('Best RIDGE alpha:     ', round(ridge.alpha_, 2))\n",
    "print('')\n",
    "print('Testing LASSO r2:     ', round(r2_score(y, lasso_pred),2))\n",
    "print('Testing Ridge r2:     ', round(r2_score(y, ridge_pred),2))\n",
    "\n",
    "print('Ridge train MAE', mae(y, ridge_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Urbanization / regional factors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x = df[['RUCC_2013','DENSITY','CENSUS_DIV_region 2', 'CENSUS_DIV_region 3',\n",
    "       'CENSUS_DIV_region 4', 'CENSUS_DIV_region 5', 'CENSUS_DIV_region 6',\n",
    "       'CENSUS_DIV_region 7', 'CENSUS_DIV_region 8', 'CENSUS_DIV_region 9']]\n",
    "\n",
    "y = df['MEDIAN_VAL']\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "x,y = np.array(x), np.array(y)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "alphavec = np.linspace(2,100,100) # I ran this test a few times to find the best range for alphas\n",
    "\n",
    "# performing regression\n",
    "lasso = LassoCV(alphas = alphavec, cv= 5)\n",
    "ridge = RidgeCV(alphas = alphavec, cv= 5)\n",
    "\n",
    "lasso.fit(x,y)\n",
    "ridge.fit(x,y)\n",
    "\n",
    "lasso_pred = lasso.predict(x)\n",
    "ridge_pred = ridge.predict(x)\n",
    "\n",
    "print('Best LASSO alpha:     ', round(lasso.alpha_, 2))\n",
    "print('Best RIDGE alpha:     ', round(ridge.alpha_, 2))\n",
    "print('')\n",
    "print('Val LASSO r2:         ', round(r2_score(y, lasso_pred),2))\n",
    "print('Val Ridge r2:         ', round(r2_score(y, ridge_pred),2))\n",
    "print(mae(y, ridge_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. All factors combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x = df[['JAN_TEMP', 'JAN_SUN',\n",
    "       'JULY_TEMP', 'HUMIDITY', 'TOPO', 'WATER', 'LN_WATER', 'JAN TEMP - Z',\n",
    "       'JAN SUN - Z', 'JUL TEMP - Z', 'JUL HUM - Z', 'TOPOG - Z',\n",
    "       'LN WATER  AREA - Z', 'NAT_AMENITY', 'RANK', 'RUCC_2013', 'DENSITY',\n",
    "       'CBSA', 'CENSUS_DIV_region 2', 'CENSUS_DIV_region 3',\n",
    "       'CENSUS_DIV_region 4', 'CENSUS_DIV_region 5', 'CENSUS_DIV_region 6',\n",
    "       'CENSUS_DIV_region 7', 'CENSUS_DIV_region 8', 'CENSUS_DIV_region 9']]\n",
    "\n",
    "y = df['MEDIAN_VAL']\n",
    "\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "#x_tr,y_tr = np.array(x_tr), np.array(y_tr)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_tr)\n",
    "x_tr = scaler.transform(x_tr)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "\n",
    "alphavec = np.linspace(0.4,4,100) # I ran this test a few times to find the best range for alphas\n",
    "\n",
    "lasso = LassoCV(alphas = alphavec, cv= 5)\n",
    "ridge = RidgeCV(alphas = alphavec, cv= 5)\n",
    "\n",
    "lasso.fit(x_tr,y_tr)\n",
    "ridge.fit(x_tr,y_tr)\n",
    "\n",
    "lasso_tr_pred = lasso.predict(x_tr)\n",
    "lasso_test_pred = lasso.predict(x_test)\n",
    "\n",
    "ridge_tr_pred = ridge.predict(x_tr)\n",
    "ridge_test_pred = ridge.predict(x_test)\n",
    "\n",
    "print('Best LASSO alpha:     ', round(lasso.alpha_, 2))\n",
    "print('Best RIDGE alpha:     ', round(ridge.alpha_, 2))\n",
    "print('')\n",
    "print('Training LASSO r2:    ', round(r2_score(y_tr, lasso_tr_pred),2))\n",
    "print('Testing LASSO r2:     ', round(r2_score(y_test, lasso_test_pred),2))\n",
    "print('Testing LASSO MAE:    ', round(mae(y_test, lasso_test_pred),2))\n",
    "print('')\n",
    "print('Training Ridge r2:    ', round(r2_score(y_tr, ridge_tr_pred),2))\n",
    "print('Testing Ridge r2:     ', round(r2_score(y_test, ridge_test_pred),2))\n",
    "print('Training Ridge MAE:   ', round(mae(y_tr, ridge_tr_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['JAN_TEMP', 'JAN_SUN',\n",
    "       'JULY_TEMP', 'HUMIDITY', 'TOPO', 'WATER', 'LN_WATER', 'JAN TEMP - Z',\n",
    "       'JAN SUN - Z', 'JUL TEMP - Z', 'JUL HUM - Z', 'TOPOG - Z',\n",
    "       'LN WATER  AREA - Z', 'NAT_AMENITY', 'RANK', 'RUCC_2013', 'DENSITY',\n",
    "       'CBSA', 'CENSUS_DIV_region 2', 'CENSUS_DIV_region 3',\n",
    "       'CENSUS_DIV_region 4', 'CENSUS_DIV_region 5', 'CENSUS_DIV_region 6',\n",
    "       'CENSUS_DIV_region 7', 'CENSUS_DIV_region 8', 'CENSUS_DIV_region 9']\n",
    "\n",
    "coef = list(zip(cols,ridge.coef_))\n",
    "#print(coef)\n",
    "\n",
    "for i in range(len(coef)):\n",
    "    print( coef[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, ridge_test_pred)\n",
    "plt.title('Predicted and Actual Home Prices by City');\n",
    "plt.xlabel('Predicted cost (thousands of dollars)');\n",
    "plt.ylabel('Actual cost (thousands of dollars)');\n",
    "#plt.savefig('predicted_actual_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test.values, y_test.index)\n",
    "plt.scatter(ridge_test_pred, y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics for model 3 (ridge, all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphavec = np.linspace(0.1,3,100)\n",
    "ridge = RidgeCV(alphas = alphavec, cv=5)\n",
    "ridge.fit(x_tr, y_tr)\n",
    "ridge.score(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_df = pd.DataFrame()\n",
    "diag_df['ACTUAL'] = y_tr\n",
    "diag_df['PRED'] = ridge.predict(x_tr)\n",
    "diag_df['RESID'] = diag_df['ACTUAL'] - diag_df['PRED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_df.plot(kind='scatter',\n",
    "            x='PRED',y='ACTUAL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(diag_df.PRED, diag_df.ACTUAL)\n",
    "plt.title('Predicted and Actual Home Prices by City');\n",
    "plt.xlabel('Predicted cost (thousands of dollars)');\n",
    "plt.ylabel('Actual cost (thousands of dollars)');\n",
    "plt.savefig('predicted_actual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.probplot(diag_df['RESID'], dist='norm', plot=plt)\n",
    "plt.title('Normal Q-Q plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be a few outliers at the high end of the range that are throwing the model off. Let's investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping outliers\n",
    "df[df.MEDIAN_VAL > 600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping outliers\n",
    "We want to drop SANTA CLARA, SAN MATEO, and SAN DIEGO. They are the three outliers that are a lot more expensive than our model thinks they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = df\n",
    "#dropout = dropout.drop(dropout[dropout['COUNTY'] == 'SANTA CLARA'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = dropout.drop(index=[15,16,17])\n",
    "#dropout.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on data without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ridge on dropout\n",
    "# splitting data\n",
    "x = dropout[['JAN_TEMP', 'JAN_SUN',\n",
    "       'JULY_TEMP', 'HUMIDITY', 'TOPO', 'WATER', 'LN_WATER', 'JAN TEMP - Z',\n",
    "       'JAN SUN - Z', 'JUL TEMP - Z', 'JUL HUM - Z', 'TOPOG - Z',\n",
    "       'LN WATER  AREA - Z', 'NAT_AMENITY', 'RANK', 'RUCC_2013', 'DENSITY',\n",
    "       'CBSA', 'CENSUS_DIV_region 2', 'CENSUS_DIV_region 3',\n",
    "       'CENSUS_DIV_region 4', 'CENSUS_DIV_region 5', 'CENSUS_DIV_region 6',\n",
    "       'CENSUS_DIV_region 7', 'CENSUS_DIV_region 8', 'CENSUS_DIV_region 9']]\n",
    "\n",
    "y = dropout['MEDIAN_VAL']\n",
    "\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "x_tr,y_tr = np.array(x), np.array(y)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_tr)\n",
    "x_tr = scaler.transform(x_tr)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "alphavec = np.linspace(0.4,4,100) # I ran this test a few times to find the best range for alphas\n",
    "\n",
    "lasso = LassoCV(alphas = alphavec, cv= 5)\n",
    "ridge = RidgeCV(alphas = alphavec, cv= 5)\n",
    "\n",
    "lasso.fit(x_tr,y_tr)\n",
    "ridge.fit(x_tr,y_tr)\n",
    "\n",
    "lasso_tr_pred = lasso.predict(x_tr)\n",
    "lasso_test_pred = lasso.predict(x_test)\n",
    "\n",
    "ridge_tr_pred = ridge.predict(x_tr)\n",
    "ridge_test_pred = ridge.predict(x_test)\n",
    "\n",
    "print('Best LASSO alpha:', round(lasso.alpha_, 2))\n",
    "print('Best RIDGE alpha:', round(ridge.alpha_, 2))\n",
    "print('')\n",
    "print('Training LASSO r2:', round(r2_score(y_tr, lasso_tr_pred),2))\n",
    "print('Testing LASSO r2:', round(r2_score(y_test, lasso_test_pred),2))\n",
    "print('Testing LASSO MAE:', round(mae(y_test, lasso_test_pred),2))\n",
    "print('')\n",
    "print('Training Ridge r2:', round(r2_score(y_tr, ridge_tr_pred),2))\n",
    "print('Testing Ridge r2:', round(r2_score(y_test, ridge_test_pred),2))\n",
    "print('Testing Ridge MAE:', round(mae(y_test, ridge_test_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing the three outlier cities, we have increased the Ridge r2 from 0.52 to 0.55, and reduced MAE from 64,230 to 48,480. Interestingly, the LASSO regression now outperforms the Ridge slightly. It's r2 is now 0.57, with a MAE of 46,750..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostics on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_df = pd.DataFrame()\n",
    "diag_df['ACTUAL'] = y_tr\n",
    "diag_df['PRED'] = ridge.predict(x_tr)\n",
    "diag_df['RESID'] = diag_df['ACTUAL'] - diag_df['PRED']\n",
    "\n",
    "diag_df.plot(kind='scatter',\n",
    "            x='PRED',y='ACTUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(diag_df['PRED'], diag_df['ACTUAL'])\n",
    "plt.title('Predicted and Actual Home Prices by City')\n",
    "plt.xlabel('Predicted cost (thousands of dollars)')\n",
    "plt.ylabel('Actual cost (thousands of dollars)')\n",
    "plt.savefig('predicted_actual_no_outliers_added_exp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(diag_df['RESID'], dist='norm', plot=plt)\n",
    "plt.title('Normal Q-Q plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, there is still a curve to the Q-Q plot. Maybe I need to add an interaction feature somewhere. The highly rated cities are more valuable than they should be still. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding exponential feature\n",
    "I want to capture the pattern that cities that are predicted to be expensive tend to be even more expensive than the model would predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout['NAT2'] = dropout['NAT_AMENITY']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing eda with exponential feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.heatmap(df.corr(), cmap=\"seismic\", vmin=-1, vmax=1, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding boxcox transformation\n",
    "dropout['TRANSFORMED_VAL'] = 1/np.sqrt(dropout['MEDIAN_VAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with exponential feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ridge on dropout\n",
    "# splitting data\n",
    "x = dropout[['JAN_TEMP', 'JAN_SUN',\n",
    "       'JULY_TEMP', 'HUMIDITY', 'TOPO', 'WATER', 'LN_WATER', 'JAN TEMP - Z',\n",
    "       'JAN SUN - Z', 'JUL TEMP - Z', 'JUL HUM - Z', 'TOPOG - Z',\n",
    "       'LN WATER  AREA - Z', 'NAT_AMENITY', 'RANK', 'RUCC_2013', 'DENSITY',\n",
    "       'CBSA', 'CENSUS_DIV_region 2', 'CENSUS_DIV_region 3',\n",
    "       'CENSUS_DIV_region 4', 'CENSUS_DIV_region 5', 'CENSUS_DIV_region 6',\n",
    "       'CENSUS_DIV_region 7', 'CENSUS_DIV_region 8', 'CENSUS_DIV_region 9','NAT2']]\n",
    "\n",
    "y = dropout['TRANSFORMED_VAL']\n",
    "\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "x_tr,y_tr = np.array(x), np.array(y)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_tr)\n",
    "x_tr = scaler.transform(x_tr)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "alphavec = 10**np.linspace(-2,2,200) # I ran this test a few times to find the best range for alphas\n",
    "\n",
    "lasso = LassoCV(alphas = alphavec, cv= 5)\n",
    "ridge = RidgeCV(alphas = alphavec, cv= 5)\n",
    "\n",
    "lasso.fit(x_tr,y_tr)\n",
    "ridge.fit(x_tr,y_tr)\n",
    "\n",
    "lasso_tr_pred = lasso.predict(x_tr)\n",
    "lasso_test_pred = lasso.predict(x_test)\n",
    "\n",
    "ridge_tr_pred = ridge.predict(x_tr)\n",
    "ridge_test_pred = ridge.predict(x_test)\n",
    "\n",
    "#print('Best LASSO alpha:', round(lasso.alpha_, 2))\n",
    "print('Best RIDGE alpha:', round(ridge.alpha_, 2))\n",
    "#print('')\n",
    "#print('Training LASSO r2:', round(r2_score(y_tr, lasso_tr_pred),2))\n",
    "#print('Testing LASSO r2:', round(r2_score(y_test, lasso_test_pred),2))\n",
    "#print('Testing LASSO MAE:', round(mae(y_test, lasso_test_pred),2))\n",
    "#print('')\n",
    "print('Training Ridge r2:', round(r2_score(y_tr, ridge_tr_pred),2))\n",
    "print('Testing Ridge r2:', round(r2_score(y_test, ridge_test_pred),2))\n",
    "print('Training Ridge MAE:', round(mae(y_tr, ridge_tr_pred),2))\n",
    "print('Testing Ridge MAE:', round(mae(y_test, ridge_test_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ridge on dropout\n",
    "# splitting data\n",
    "x = dropout[['JAN_TEMP', 'JAN_SUN',\n",
    "       'JULY_TEMP', 'HUMIDITY', 'TOPO', 'WATER', 'LN_WATER', 'JAN TEMP - Z',\n",
    "       'JAN SUN - Z', 'JUL TEMP - Z', 'JUL HUM - Z', 'TOPOG - Z',\n",
    "       'LN WATER  AREA - Z', 'NAT_AMENITY', 'RANK', 'NAT2']]\n",
    "\n",
    "y = dropout['TRANSFORMED_VAL']\n",
    "\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_tr.values)\n",
    "x_tr = scaler.transform(x_tr.values)\n",
    "x_test = scaler.transform(x_test.values)\n",
    "\n",
    "alphavec = 10**np.linspace(-4,4,100) # I ran this test a few times to find the best range for alphas\n",
    "\n",
    "lasso = LassoCV(alphas = alphavec, cv= 5)\n",
    "ridge = RidgeCV(alphas = alphavec, cv= 5)\n",
    "\n",
    "lasso.fit(x_tr,y_tr)\n",
    "ridge.fit(x_tr,y_tr)\n",
    "\n",
    "lasso_tr_pred = lasso.predict(x_tr)\n",
    "lasso_test_pred = lasso.predict(x_test)\n",
    "\n",
    "ridge_tr_pred = ridge.predict(x_tr)\n",
    "ridge_test_pred = ridge.predict(x_test)\n",
    "\n",
    "#print('Best LASSO alpha:', round(lasso.alpha_, 2))\n",
    "print('Best RIDGE alpha:', ridge.alpha_)\n",
    "#print('')\n",
    "#print('Training LASSO r2:', round(r2_score(y_tr, lasso_tr_pred),2))\n",
    "#print('Testing LASSO r2:', round(r2_score(y_test, lasso_test_pred),2))\n",
    "#print('Testing LASSO MAE:', round(mae(y_test, lasso_test_pred),2))\n",
    "#print('')\n",
    "print('Training Ridge r2:', round(r2_score(y_tr, ridge_tr_pred),2))\n",
    "print('Testing Ridge r2:', round(r2_score(y_test, ridge_test_pred),2))\n",
    "print('Training Ridge MAE:', round(mae(y_tr, ridge_tr_pred),2))\n",
    "print('Testing Ridge MAE:', round(mae(y_test, ridge_test_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostics on data without outliers, and with exponential term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_df = pd.DataFrame()\n",
    "diag_df['ACTUAL'] = y_tr\n",
    "diag_df['PRED'] = ridge.predict(x_tr)\n",
    "diag_df['RESID'] = diag_df['ACTUAL'] - diag_df['PRED']\n",
    "\n",
    "diag_df.plot(kind='scatter',\n",
    "            x='PRED',y='ACTUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(diag_df['PRED'], diag_df['ACTUAL'])\n",
    "plt.title('Predicted and Actual Home Prices by City')\n",
    "plt.xlabel('Predicted cost (thousands of dollars)')\n",
    "plt.ylabel('Actual cost (thousands of dollars)')\n",
    "plt.savefig('predicted_actual_no_outliers_added_exp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(diag_df['RESID'], dist='norm', plot=plt)\n",
    "plt.title('Normal Q-Q plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dropout.to_pickle('/Users/patricknorman/Documents/GitHub/metis-project-2/data/dropout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying BoxCox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.boxcox(dropout['MEDIAN_VAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
